{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from dbn.tensorflow import SupervisedDBNClassification\n",
    "from dbn.tensorflow.models import UnsupervisedDBN\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv, re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.classification import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import itertools\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn import svm, tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "import time\n",
    "import re\n",
    "\n",
    "import eli5\n",
    "\n",
    "pd.set_option('display.max_rows', None) # display all rows\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processTokensOfOneFile( oneFileContent ):\n",
    "    stemmer_obj  = SnowballStemmer(\"english\")\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and\n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    \n",
    "    # Remove non-letters\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", oneFileContent)\n",
    "    \n",
    "    # Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()\n",
    "    \n",
    "    # In Python, searching a set is much faster than searching a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    # Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    \n",
    "    # Only inlcude words at least of length 3\n",
    "    valid_len_words = [w for w in meaningful_words if len(w) >= 3]\n",
    "        \n",
    "    # convert words to utf\n",
    "    stemmed_words = [stemmer_obj.stem(token) for token in valid_len_words]\n",
    "    \n",
    "    #Join the words back into one string separated by space, and return the result.\n",
    "    join_words =  \" \".join( stemmed_words )\n",
    "\n",
    "    # More text preprocessing, tokenizing and filtering of stopwords \n",
    "    # Not really needed\n",
    "    try:\n",
    "        count_vect = CountVectorizer() \n",
    "        more_processed_words = count_vect.fit_transform(join_words)\n",
    "        return more_processed_words\n",
    "    except ValueError:\n",
    "        return join_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def giveFileContent(fileNameParam):\n",
    "    str2ret=\"\"\n",
    "    for line_ in open(\"..//\" + re.split('V5/', fileNameParam)[1], 'rU'):\n",
    "        li=line_.strip()\n",
    "        str2ret = str2ret + line_.rstrip()\n",
    "    return str2ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTokensForTokenization(file_name):\n",
    "    completeCorpus    = [] ## a list of lists with tokens from defected and non defected files\n",
    "    for fileToRead in file_name:\n",
    "        fileContentAsStr = giveFileContent(fileToRead)\n",
    "        filtered_str_from_one_file = processTokensOfOneFile(fileContentAsStr)\n",
    "        completeCorpus.append(filtered_str_from_one_file)       \n",
    "    return completeCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_performance(true_label, predicted_label):   \n",
    "    precision = recall = f1 = np.zeros(2, dtype=np.float32)\n",
    "    report = classification_report(true_label, predicted_label, digits=3)\n",
    "    precision = precision_score(true_label, predicted_label, average=None, labels=[0,1])\n",
    "    recall = recall_score(true_label, predicted_label, average=None, labels=[0,1])\n",
    "    f1 = f1_score(true_label, predicted_label, average=None, labels=[0,1])\n",
    "    return recall, precision, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLabelEncoder(object):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        It differs from LabelEncoder by handling new classes and providing a value for it [Unknown]\n",
    "        Unknown will be added in fit and transform will take care of new item. It gives unknown class id\n",
    "        \"\"\"\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        # self.classes_ = self.label_encoder.classes_\n",
    "\n",
    "    def fit(self, data_list):\n",
    "        \"\"\"\n",
    "        This will fit the encoder for all the unique values and introduce unknown value\n",
    "        :param data_list: A list of string\n",
    "        :return: self\n",
    "        \"\"\"\n",
    "        self.label_encoder = self.label_encoder.fit(list(data_list) + ['Unknown'])\n",
    "        self.classes_ = self.label_encoder.classes_\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_list):\n",
    "        \"\"\"\n",
    "        This will transform the data_list to id list where the new values get assigned to Unknown class\n",
    "        :param data_list:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        new_data_list = list(data_list)\n",
    "        for unique_item in np.unique(data_list):\n",
    "            if unique_item not in self.label_encoder.classes_:\n",
    "                new_data_list = ['Unknown' if x==unique_item else x for x in new_data_list]\n",
    "\n",
    "        return self.label_encoder.transform(new_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_source_code(file_name):     \n",
    "    unfilteredTokensFromFile = getTokensForTokenization(file_name)\n",
    "    return unfilteredTokensFromFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_vector_of_nodes(docs):\n",
    "    #documents\n",
    "    #docs = ['if foo for bar car', 'foo for if bar']\n",
    "    \n",
    "    docs = [x.lower() for x in docs]\n",
    "  \n",
    "    # split documents to tokens\n",
    "    words = [doc.split(\" \") for doc in docs]\n",
    "    #print(\"All tokens: \", words)\n",
    "    \n",
    "    # find the length of vectors\n",
    "    max_len = np.max([len(x) for x in words])\n",
    "    #print(\"Vector Length: \", max_len)\n",
    "    \n",
    "    # convert list of of token-lists to one flat list of tokens\n",
    "    flatten_words = list(itertools.chain.from_iterable(words))\n",
    "    #print(\"Flatten tokens: \", flatten_words)\n",
    "    \n",
    "    #fine all the unique tokens\n",
    "    unique_words = np.unique(flatten_words)\n",
    "    print(\"Feature Number: \", unique_words.size)\n",
    "    #print(\"Unique tokens: \", unique_words)\n",
    "    \n",
    "    # filter once again to inlcude words at least of length 3\n",
    "    invalid_words = []\n",
    "    for word in unique_words:\n",
    "        if len(word) < 3:\n",
    "            invalid_words.append(word) \n",
    "    unique_words = [x for x in unique_words if x not in invalid_words]\n",
    "    unique_words = np.array(unique_words)\n",
    "    \n",
    "    # integer encode\n",
    "    encoded_docs = []\n",
    "    label_encoder = MyLabelEncoder()\n",
    "    label_encoder.fit(unique_words)\n",
    "    for doc in docs:\n",
    "        #print(doc)\n",
    "        words = doc.split(\" \")\n",
    "        #print(words)\n",
    "        integer_encoded = label_encoder.transform(words)\n",
    "        integer_encoded = np.pad(integer_encoded, (0, max_len - len(integer_encoded))) #padding with 0 to make fixed sized vectors\n",
    "        #print(integer_encoded)\n",
    "        #print(\"**************\")\n",
    "        encoded_docs.append(integer_encoded)\n",
    "    \n",
    "    #print(encoded_docs)\n",
    "    return encoded_docs, unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom Transformer that extracts components with loading >=0 of PCA \n",
    "class FeatureSelector( BaseEstimator, TransformerMixin ):\n",
    "    #Class Constructor \n",
    "    def __init__( self, feature_names, label ):\n",
    "        self.feature_names = feature_names \n",
    "        self.label = label\n",
    "    \n",
    "    #Return self nothing else to do here    \n",
    "    def fit( self, X, y = None ):\n",
    "        return self \n",
    "    \n",
    "    #Method that describes what we need this transformer to do\n",
    "    def transform( self, X, y = None ):\n",
    "        print(\"Before applying PCA data size: \", X.shape)\n",
    "        pca = PCA(0.95)\n",
    "        pca.fit(X, y)\n",
    "        X = pca.transform(X)\n",
    "        print(\"After applying PCA data size: \", X.shape)\n",
    "        print(X)\n",
    "        print(pca.components_)\n",
    "        cut_component = np.array([x for x in pca.components_ if x.any() >0])\n",
    "        print(cut_component)   \n",
    "        return cut_component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_extractor(clf, feature_list):\n",
    "    dbn = UnsupervisedDBN(hidden_layers_structure=[feature_list.size, feature_list.size],\n",
    "                              batch_size=1,\n",
    "                              learning_rate_rbm=0.06,\n",
    "                              n_epochs_rbm=1,\n",
    "                              activation_function='sigmoid',\n",
    "                              verbose =0)\n",
    "    classifier = Pipeline(steps=[('dbn', dbn), ('clf', clf)], verbose= True)  \n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_classifier(clf, feature_list):\n",
    "    dbn = UnsupervisedDBN(hidden_layers_structure=[feature_list.size, feature_list.size],\n",
    "                              batch_size=1,\n",
    "                              learning_rate_rbm=0.06,\n",
    "                              n_epochs_rbm=1,\n",
    "                              activation_function='sigmoid',\n",
    "                              verbose =1)\n",
    "    pca = PCA(0.95) #  choose the minimum number of principal components such that 95% of the variance is retained.\n",
    "    classifier = Pipeline(steps=[('dbn', dbn), ('pca', pca), ('clf', clf)], verbose= True)  \n",
    "    #classifier = Pipeline(steps=[('dbn', dbn), (\"select_feature\", FeatureSelector(feature_list, true_label)), ('clf', clf)], verbose= True)  \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_cv(data, true_label, feature_list):\n",
    "    \n",
    "    data = np.array(data)\n",
    "    \n",
    "    # 10 fold cv\n",
    "    kf = KFold(n_splits=10, shuffle = True, random_state = 7)\n",
    "\n",
    "    cv_recall_DT = []\n",
    "    cv_precision_DT = []\n",
    "    cv_f1_DT = []\n",
    "    cv_fit_time_DT = []\n",
    "    cv_predict_time_DT = []\n",
    "    \n",
    "    cv_recall_KNN = []\n",
    "    cv_precision_KNN = []\n",
    "    cv_f1_KNN = []\n",
    "    cv_fit_time_KNN = []\n",
    "    cv_predict_time_KNN = []\n",
    "    \n",
    "    cv_recall_SVM = []\n",
    "    cv_precision_SVM = []\n",
    "    cv_f1_SVM = []\n",
    "    cv_fit_time_SVM = []\n",
    "    cv_predict_time_SVM = []\n",
    "    \n",
    "    cv_recall_NB = []\n",
    "    cv_precision_NB = []\n",
    "    cv_f1_NB = []\n",
    "    cv_fit_time_NB = []\n",
    "    cv_predict_time_NB = []\n",
    "    \n",
    "    cv_recall_RF = []\n",
    "    cv_precision_RF = []\n",
    "    cv_f1_RF = []\n",
    "    cv_fit_time_RF =[]\n",
    "    cv_predict_time_RF = []\n",
    "\n",
    "\n",
    "    for train_index, test_index in kf.split(data):        \n",
    "        train, test = data[train_index], data[test_index]\n",
    "        train_label, test_label = true_label[train_index], true_label[test_index]\n",
    "        \n",
    "        #train, test = apply_PCA(train, test)\n",
    "         \n",
    "        clf = tree.DecisionTreeClassifier(criterion='gini', splitter='best', \n",
    "                                       min_samples_split = 2, min_weight_fraction_leaf=0.0)\n",
    "        classifier = semantic_classifier(clf, feature_list)\n",
    "        start = time.perf_counter()\n",
    "        classifier.fit(train, train_label)\n",
    "        end = time.perf_counter()\n",
    "        fit_time = end - start\n",
    "        start = time.perf_counter()\n",
    "        predicted_label = classifier.predict(test)\n",
    "        end = time.perf_counter()\n",
    "        predict_time = end - start\n",
    "        recall, precision, f1 = measure_performance(test_label, predicted_label)\n",
    "        cv_recall_DT.append(recall)\n",
    "        cv_precision_DT.append(precision)\n",
    "        cv_f1_DT.append(f1)\n",
    "        cv_fit_time_DT.append(fit_time)\n",
    "        cv_predict_time_DT.append(predict_time)  \n",
    "\n",
    "        \n",
    "        clf = KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
    "        classifier = semantic_classifier(clf, feature_list)\n",
    "        start = time.perf_counter()\n",
    "        classifier.fit(train, train_label)\n",
    "        end = time.perf_counter()\n",
    "        fit_time = end - start\n",
    "        start = time.perf_counter()\n",
    "        predicted_label = classifier.predict(test)\n",
    "        end = time.perf_counter()\n",
    "        predict_time = end - start\n",
    "        recall, precision, f1 = measure_performance(test_label, predicted_label)\n",
    "        cv_recall_KNN.append(recall)\n",
    "        cv_precision_KNN.append(precision)\n",
    "        cv_f1_KNN.append(f1)\n",
    "        cv_fit_time_KNN.append(fit_time)\n",
    "        cv_predict_time_KNN.append(predict_time)\n",
    "\n",
    "        \n",
    "        clf = svm.SVC(gamma='auto', C = 20.0, kernel='rbf')\n",
    "        classifier = semantic_classifier(clf, feature_list)\n",
    "        start = time.perf_counter()\n",
    "        classifier.fit(train, train_label)\n",
    "        end = time.perf_counter()\n",
    "        fit_time = end - start\n",
    "        start = time.perf_counter()\n",
    "        predicted_label = classifier.predict(test)\n",
    "        end = time.perf_counter()\n",
    "        predict_time = end - start\n",
    "        recall, precision, f1 = measure_performance(test_label, predicted_label)\n",
    "        cv_recall_SVM.append(recall)\n",
    "        cv_precision_SVM.append(precision)\n",
    "        cv_f1_SVM.append(f1)\n",
    "        cv_fit_time_SVM.append(fit_time)\n",
    "        cv_predict_time_SVM.append(predict_time)\n",
    "        \n",
    "        \n",
    "        clf = GaussianNB()\n",
    "        classifier = semantic_classifier(clf, feature_list)\n",
    "        start = time.perf_counter()\n",
    "        classifier.fit(train, train_label)\n",
    "        end = time.perf_counter()\n",
    "        fit_time = end - start\n",
    "        start = time.perf_counter()\n",
    "        predicted_label = classifier.predict(test)\n",
    "        end = time.perf_counter()\n",
    "        predict_time = end - start\n",
    "        recall, precision, f1 = measure_performance(test_label, predicted_label)\n",
    "        cv_recall_NB.append(recall)\n",
    "        cv_precision_NB.append(precision)\n",
    "        cv_f1_NB.append(f1)\n",
    "        cv_fit_time_NB.append(fit_time)\n",
    "        cv_predict_time_NB.append(predict_time)     \n",
    "        \n",
    "        \n",
    "        clf = RandomForestClassifier(n_estimators=10, criterion='gini')\n",
    "        classifier = semantic_classifier(clf, feature_list)\n",
    "        start = time.perf_counter()\n",
    "        classifier.fit(train, train_label)\n",
    "        end = time.perf_counter()\n",
    "        fit_time = end - start\n",
    "        start = time.perf_counter()\n",
    "        predicted_label = classifier.predict(test)\n",
    "        end = time.perf_counter()\n",
    "        predict_time = end - start\n",
    "        recall, precision, f1 = measure_performance(test_label, predicted_label)\n",
    "        cv_recall_RF.append(recall)\n",
    "        cv_precision_RF.append(precision)\n",
    "        cv_f1_RF.append(f1)\n",
    "        cv_fit_time_RF.append(fit_time)\n",
    "        cv_predict_time_RF.append(predict_time)\n",
    "\n",
    "        \n",
    "    recall_DT = np.mean(cv_recall_DT, axis= 0)\n",
    "    precision_DT = np.mean(cv_precision_DT, axis= 0)\n",
    "    f1_DT = np.mean(cv_f1_DT, axis= 0)\n",
    "    fit_time_DT = np.mean(cv_fit_time_DT)\n",
    "    predict_time_DT = np.mean(cv_predict_time_DT)\n",
    "\n",
    "    recall_KNN = np.mean(cv_recall_KNN, axis= 0)\n",
    "    precision_KNN = np.mean(cv_precision_KNN, axis= 0)\n",
    "    f1_KNN = np.mean(cv_f1_KNN, axis= 0)\n",
    "    fit_time_KNN = np.mean(cv_fit_time_KNN)\n",
    "    predict_time_KNN = np.mean(cv_predict_time_KNN)\n",
    "    \n",
    "    recall_SVM = np.mean(cv_recall_SVM, axis= 0)\n",
    "    precision_SVM = np.mean(cv_precision_SVM, axis= 0)\n",
    "    f1_SVM =  np.mean(cv_f1_SVM, axis= 0)\n",
    "    fit_time_SVM = np.mean(cv_fit_time_SVM)\n",
    "    predict_time_SVM = np.mean(cv_predict_time_SVM)\n",
    "    \n",
    "    recall_NB = np.mean(cv_recall_NB, axis= 0)\n",
    "    precision_NB = np.mean(cv_precision_NB, axis= 0)\n",
    "    f1_NB = np.mean(cv_f1_NB, axis= 0)\n",
    "    fit_time_NB = np.mean(cv_fit_time_NB)\n",
    "    predict_time_NB = np.mean(cv_predict_time_NB)\n",
    "    \n",
    "    recall_RF = np.mean(cv_recall_RF, axis= 0)\n",
    "    precision_RF = np.mean(cv_precision_RF, axis= 0)\n",
    "    f1_RF = np.mean(cv_f1_RF, axis= 0)\n",
    "    fit_time_RF = np.mean(cv_fit_time_RF)\n",
    "    predict_time_RF = np.mean(cv_predict_time_RF)\n",
    "    \n",
    "    return recall_DT, precision_DT, f1_DT, fit_time_DT, predict_time_DT, recall_KNN, precision_KNN, f1_KNN,\\\n",
    "    fit_time_KNN, predict_time_KNN, recall_SVM, precision_SVM, f1_SVM, fit_time_SVM, predict_time_SVM, recall_NB,\\\n",
    "    precision_NB, f1_NB, fit_time_NB, predict_time_NB, recall_RF, precision_RF, f1_RF, fit_time_RF, predict_time_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeated_test(data, true_label, unique_words):\n",
    "    repeated_recall_DT = []\n",
    "    repeated_precision_DT = []\n",
    "    repeated_f1_DT = []\n",
    "    repeated_fit_time_DT = []\n",
    "    repeated_predict_time_DT = []\n",
    "    \n",
    "    repeated_recall_KNN = []\n",
    "    repeated_precision_KNN = []\n",
    "    repeated_f1_KNN = []\n",
    "    repeated_fit_time_KNN = []\n",
    "    repeated_predict_time_KNN = []\n",
    "    \n",
    "    repeated_recall_SVM = []\n",
    "    repeated_precision_SVM = []\n",
    "    repeated_f1_SVM = []\n",
    "    repeated_fit_time_SVM = []\n",
    "    repeated_predict_time_SVM = []\n",
    "    \n",
    "    repeated_recall_NB = []\n",
    "    repeated_precision_NB = []\n",
    "    repeated_f1_NB = []\n",
    "    repeated_fit_time_NB = []\n",
    "    repeated_predict_time_NB = []\n",
    "    \n",
    "    repeated_recall_RF = []\n",
    "    repeated_precision_RF = []\n",
    "    repeated_f1_RF = []\n",
    "    repeated_fit_time_RF = []\n",
    "    repeated_predict_time_RF = []\n",
    "    \n",
    "    recall_DT= precision_DT= f1_DT= fit_time_DT= predict_time_DT= recall_KNN= precision_KNN= f1_KNN=\\\n",
    "    fit_time_KNN= predict_time_KNN= recall_SVM = precision_SVM= f1_SVM= fit_time_SVM= predict_time_SVM\\\n",
    "    = recall_NB= precision_NB= f1_NB= fit_time_NB= predict_time_NB= recall_RF= precision_RF= f1_RF\\\n",
    "    = fit_time_RF= predict_time_RF = 0\n",
    "    \n",
    "    for i in range(10):\n",
    "        recall_DT, precision_DT, f1_DT, fit_time_DT, predict_time_DT, recall_KNN, precision_KNN, f1_KNN,\\\n",
    "        fit_time_KNN, predict_time_KNN, recall_SVM, precision_SVM, f1_SVM, fit_time_SVM, predict_time_SVM,\\\n",
    "        recall_NB, precision_NB, f1_NB, fit_time_NB, predict_time_NB, recall_RF, precision_RF, f1_RF, fit_time_RF,\\\n",
    "        predict_time_RF = kfold_cv(data, true_label, unique_words)\n",
    "        \n",
    "        repeated_recall_DT.append(recall_DT)\n",
    "        repeated_precision_DT.append(precision_DT)\n",
    "        repeated_f1_DT.append(f1_DT)\n",
    "        repeated_fit_time_DT.append(fit_time_DT) \n",
    "        repeated_predict_time_DT.append(predict_time_DT)\n",
    "\n",
    "        repeated_recall_KNN.append(recall_KNN)\n",
    "        repeated_precision_KNN.append(precision_KNN)\n",
    "        repeated_f1_KNN.append(f1_KNN)\n",
    "        repeated_fit_time_KNN.append(fit_time_KNN) \n",
    "        repeated_predict_time_KNN.append(predict_time_KNN)\n",
    "\n",
    "        repeated_recall_SVM.append(recall_SVM)\n",
    "        repeated_precision_SVM.append(precision_SVM)\n",
    "        repeated_f1_SVM.append(f1_SVM)\n",
    "        repeated_fit_time_SVM.append(fit_time_SVM) \n",
    "        repeated_predict_time_SVM.append(predict_time_SVM)\n",
    "        \n",
    "        repeated_recall_NB.append(recall_NB)\n",
    "        repeated_precision_NB.append(precision_NB)\n",
    "        repeated_f1_NB.append(f1_NB)\n",
    "        repeated_fit_time_NB.append(fit_time_NB) \n",
    "        repeated_predict_time_NB.append(predict_time_NB)\n",
    "        \n",
    "        repeated_recall_RF.append(recall_RF)\n",
    "        repeated_precision_RF.append(precision_RF)\n",
    "        repeated_f1_RF.append(f1_RF)\n",
    "        repeated_fit_time_RF.append(fit_time_RF) \n",
    "        repeated_predict_time_RF.append(predict_time_RF)\n",
    "        \n",
    "    print(\"-------DT-------\")\n",
    "    print(\"Recall:\", np.median(repeated_recall_DT, axis= 0))\n",
    "    print(\"Precision:\", np.median(repeated_precision_DT, axis= 0))\n",
    "    print(\"f1 score:\", np.median(repeated_f1_DT, axis= 0))\n",
    "    print(\"Fit time:\", np.median(repeated_fit_time_DT))\n",
    "    print(\"Predict time:\", np.median(repeated_predict_time_DT))\n",
    "\n",
    "    print(\"-------KNN-------\")\n",
    "    print(\"Recall:\", np.median(repeated_recall_KNN, axis= 0))\n",
    "    print(\"Precision:\", np.median(repeated_precision_KNN, axis= 0))\n",
    "    print(\"f1 score:\", np.median(repeated_f1_KNN, axis= 0))\n",
    "    print(\"Fit time:\", np.median(repeated_fit_time_KNN))\n",
    "    print(\"Predict time:\", np.median(repeated_predict_time_KNN))\n",
    "\n",
    "    print(\"-------SVM-------\")\n",
    "    print(\"Recall:\", np.median(repeated_recall_SVM, axis= 0))\n",
    "    print(\"Precision:\", np.median(repeated_precision_SVM, axis= 0))\n",
    "    print(\"f1 score:\", np.median(repeated_f1_SVM, axis= 0))\n",
    "    print(\"Fit time:\", np.median(repeated_fit_time_SVM))\n",
    "    print(\"Predict time:\", np.median(repeated_predict_time_SVM))\n",
    "    \n",
    "    print(\"-------NB-------\")\n",
    "    print(\"Recall:\", np.median(repeated_recall_NB, axis= 0))\n",
    "    print(\"Precision:\", np.median(repeated_precision_NB, axis= 0))\n",
    "    print(\"f1 score:\", np.median(repeated_f1_NB, axis= 0))\n",
    "    print(\"Fit time:\", np.median(repeated_fit_time_NB))\n",
    "    print(\"Predict time:\", np.median(repeated_predict_time_NB))\n",
    "    \n",
    "    print(\"-------RF-------\")\n",
    "    print(\"Recall:\", np.median(repeated_recall_RF, axis= 0))\n",
    "    print(\"Precision:\", np.median(repeated_precision_RF, axis= 0))\n",
    "    print(\"f1 score:\", np.median(repeated_f1_RF, axis= 0))\n",
    "    print(\"Fit time:\", np.median(repeated_fit_time_RF))\n",
    "    print(\"Predict time:\", np.median(repeated_predict_time_RF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(encoded_files, label, feature_list):\n",
    "    if np.unique(label).size > 1:\n",
    "        clf = LogisticRegression()\n",
    "    else:\n",
    "        clf = RandomForestClassifier(n_estimators=10, criterion='gini')\n",
    "    \n",
    "    classifier = semantic_extractor(clf, feature_list)\n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    encoded_files = np.asarray(encoded_files)\n",
    "    classifier.fit(encoded_files, label)\n",
    "    end = time.perf_counter()\n",
    "    extract_time = end - start\n",
    "    predicted_label = classifier.predict(encoded_files)\n",
    "    recall, precision, f1 = measure_performance(label, predicted_label)\n",
    "    print(\"----------------RF------------------\")\n",
    "    RF_features = eli5.explain_weights_df(classifier.named_steps['clf'], top=50, feature_names = feature_list)\n",
    "    print(RF_features) \n",
    "    print(\"Extract time: \", extract_time)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"f1 score:\", f1)\n",
    "    print(\"*\"*100)\n",
    "    return RF_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance(encoded_files, label, feature_list):\n",
    "    clf = RandomForestClassifier(n_estimators=10, criterion='gini')\n",
    "    classifier = semantic_classifier(clf, feature_list)\n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    encoded_files = np.asarray(encoded_files)\n",
    "    print(\"input shape: \", encoded_files.shape)\n",
    "    classifier.fit(encoded_files, label)\n",
    "    end = time.perf_counter()\n",
    "    extract_time = end - start\n",
    "#     predicted_label = classifier.predict(encoded_files)\n",
    "#     recall, precision, f1 = measure_performance(label, predicted_label)\n",
    "    print(\"----------------RF------------------\")\n",
    "    print(\"PCA analysis:\")\n",
    "    print (\"Number of selected components: \", classifier[1].n_components_)\n",
    "    print (\"variance ratio\" , classifier.named_steps['pca'].explained_variance_ratio_) \n",
    "    df = pd.DataFrame(classifier.named_steps['pca'].components_, columns=unique_words).transpose()\n",
    "    df = df.sort_values(df.columns[0], ascending = False)\n",
    "    cut_df = df.loc[df[0] > 0]\n",
    "    print(cut_df)\n",
    "    \n",
    "#     print(\"Recall:\", recall)\n",
    "#     print(\"Precision:\", precision)\n",
    "#     print(\"f1 score:\", f1)\n",
    "    print(\"*\"*100)\n",
    "    return cut_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "process_data = pd.read_csv('..//FINAL_PROCESS_METRICS.csv') \n",
    "#process_data = pd.read_csv('..//test1.csv') \n",
    "print(\"Initial process data shape: \", process_data.shape)\n",
    "code_data = pd.read_csv('..//FINAL_CODE_METRICS.csv') \n",
    "print(\"Initial code data shape: \", code_data.shape)\n",
    "\n",
    "actual_process_file_name = process_data['file_']\n",
    "actual_code_file_name = code_data['FILE_PATH']\n",
    "\n",
    "formatted_process_file_name = []\n",
    "formatted_code_file_name = []   \n",
    "\n",
    "for item in actual_process_file_name:\n",
    "    formatted_process_file_name.append(re.split('V5/', item)[1]) \n",
    "for item in actual_code_file_name:\n",
    "    formatted_code_file_name.append(re.split('V5/', item)[1])\n",
    "    \n",
    "process_data['file_'] =  formatted_process_file_name   \n",
    "code_data['FILE_PATH'] =  formatted_code_file_name   \n",
    "    \n",
    "formatted_process_file_name = set(line.strip() for line in formatted_process_file_name)\n",
    "formatted_code_file_name = set(line.strip() for line in formatted_code_file_name)    \n",
    "    \n",
    "common_file_name = []\n",
    "true_label = []\n",
    "for common_entry in formatted_process_file_name & formatted_code_file_name:\n",
    "    if common_entry:\n",
    "        process_index =  process_data[process_data['file_'] == common_entry].index[0]\n",
    "        common_file_name.append(process_data.iloc[process_index]['FILE_PATH'])\n",
    "        true_label.append(process_data.iloc[process_index]['defect_status'])\n",
    "\n",
    "true_label = np.array(true_label)\n",
    "file_name = np.array(common_file_name)\n",
    "\n",
    "parsed_files = parse_source_code(file_name)\n",
    "encoded_files, unique_words = encode_vector_of_nodes(parsed_files)\n",
    "\n",
    "repeated_test(encoded_files, true_label, unique_words) #repeat kfold 10 times and report avarage performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sample_data = pd.read_csv('..//samplelabel.csv') \n",
    "# print(\"Initial process data shape: \", sample_data.shape)\n",
    "\n",
    "# file_name = sample_data['FILE_PATH']\n",
    "# true_label = sample_data['defect_status']\n",
    "\n",
    "# # print(file_name)\n",
    "# # print(true_label)\n",
    "\n",
    "# parsed_files = parse_source_code(file_name)\n",
    "# encoded_files, unique_words = encode_vector_of_nodes(parsed_files)\n",
    "\n",
    "# print(unique_words)\n",
    "# #repeated_test(encoded_files, true_label, unique_words) #repeat kfold 10 times and report avarage performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(\"************** All File **************\")\n",
    "# #print(file_name)\n",
    "# #all_features = extract_feature(encoded_files, true_label, unique_words)\n",
    "# all_features_pca = feature_importance(encoded_files, true_label, unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # common_file_name = file_name\n",
    "# print(\"************** Non-defect File **************\")\n",
    "# non_defect_file_index = np.where(true_label == 0)[0]\n",
    "# #print(non_defect_file_index)\n",
    "# non_defect_file_name = []\n",
    "# non_defect_file_label = []\n",
    "# for i in non_defect_file_index:\n",
    "#     non_defect_file_name.append(common_file_name[i])\n",
    "#     non_defect_file_label.append(true_label[i])\n",
    "# #print(non_defect_file_name)\n",
    "# parsed_files = parse_source_code(non_defect_file_name)\n",
    "# encoded_files, unique_words = encode_vector_of_nodes(parsed_files)\n",
    "\n",
    "# # print(np.asarray(encoded_files).shape)\n",
    "# # print(np.asarray(encoded_files))\n",
    "\n",
    "# non_defect_features = extract_feature(encoded_files, non_defect_file_label, unique_words)\n",
    "# non_defect_features_pca = feature_importance(encoded_files, non_defect_file_label, unique_words, non_defect_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"************** Defect File **************\")\n",
    "# defect_file_index = np.where(true_label == 1)[0]\n",
    "# #print(defect_file_index)\n",
    "# defect_file_name = []\n",
    "# defect_file_label = []\n",
    "# for i in defect_file_index:\n",
    "#     defect_file_name.append(common_file_name[i])\n",
    "#     defect_file_label.append(true_label[i])\n",
    "# #print(defect_file_name)\n",
    "# parsed_files = parse_source_code(defect_file_name)\n",
    "# encoded_files, unique_words = encode_vector_of_nodes(parsed_files)\n",
    "\n",
    "# defect_features = extract_feature(encoded_files, defect_file_label, unique_words)\n",
    "# defect_features_pca = feature_importance(encoded_files, defect_file_label, unique_words, defect_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Unique features in defect files\")\n",
    "# diff = set(defect_features['feature']) - set(non_defect_features['feature']) \n",
    "# print(diff)\n",
    "\n",
    "# # diff = set(all_features['feature']) - set(non_defect_features['feature']) \n",
    "# # print(diff)\n",
    "\n",
    "# print(\"Unique features in non defect files\")\n",
    "# diff = set(non_defect_features['feature']) - set(defect_features['feature']) \n",
    "# print(diff)\n",
    "\n",
    "# # diff = set(all_features['feature']) - set(defect_features['feature']) \n",
    "# # print(diff)\n",
    "\n",
    "# print(\"Common features\")\n",
    "# diff = set(defect_features['feature']) & set(non_defect_features['feature'])\n",
    "# print(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
